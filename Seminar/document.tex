\documentclass{proseminar}

\let\i\undefined
\let\c\undefined
\let\b\undefined
\newcommand{\i}[1]{\emph{#1}}
\newcommand{\c}[1]{\texttt{#1}}
\newcommand{\b}[1]{\textbf{#1}}
\begin{document}

\conferenceinfo{Albert-Ludwigs Universit\"at Freiburg\\Technische Fakult\"at, Institut f\"ur Informatik\\Lehrstuhl f\"ur Datenbanken \& Informationssysteme}{}

\title{Trust and Privacy in Social Media \\
\huge Content oriented trust}

\numberofauthors{1} 
\author{
Tim Schmiedl\\
\email{tim.schmiedl@neptun.uni-freiburg.de}
}

\maketitle

\section{Introduction}
Over the last couple of years microblogging became a well-established paradigm for interaction in online social networks.
Especially the success of Twitter let to an increase of huge amount of real-time informations on the internet. It lets user post short messages which are shown immeadietly for their followers. This concept was of course quickly adapted by news agencies, product pages and many other. So nowadays Twitter displays a wide variety of content. Most tweets correspond to the following categories; conversation items, wich are just valuable to user and its immediate circle of friends, or infomation or news items, which could be interesting for a broader community.

%* mobile internet devices 
%* propagating news and information about developing events 

% =========================

\subsection*{Current Situation}
Twitter was well known to display tweets in reverse cronical order displaying what is happening right now on the very top of the news feed. 
The microblogging company itself tried to experiement in some ways to show the information in different ways, one is to show popular tweets on top regardless of the posting time.
This decision was often critized as it often shows for the user unrelevant information or advertisment. This position of popular tweet was also often the target of internet trolls or spammers trying polute the news feed of many users.

As microblogging has become more and more popular it was shown that beside chatting about events on a day-to-day basis it can be in particularly helpful during emergency and/or crisis situations.
Twitter provide real-time information from the actual location where the crisis is unfolding. This information often spreads faster and to a wider audience than what traditional news media sources can achieve.


% =========================

\subsection*{Problems}
Twitters ranking method of displaying popular tweets as not taken into account content relevance or the twitter account.
This lets a large amount of pointless tweets may flood the relevant tweets of the users news feed. It also allows for more potential for spam to spead, as it is not filtered in this ranking method.

As a result of the big variety of content and different kinds of users it is very difficult to differenciate between news related topics and chat between user. Finding tweets which have valuable information outside its immediate circle of friends or follower is often not easy as there is no distinction between different kind of messages. 


% =========================

\subsection*{Goals of the Papers}
The primary element of this report are the two papers...

In P2\cite{P2} they propose a method to rank tweets based on their content relevance to a given query. They use different set of features to find out weather a tweet is interesting for a user. Their \i{learning to rank} algorithm considers content relevance as well as account authority and twitter-specific features to rank tweets. In their evaluation they find out that especially if a URL is present in the tweet improves the relevance as well as that just the number of followers to an account doesn't directly correlates to bigger relevances of tweets from that author.

In P1\cite{P1} the authors had a the main focus on the credibility of time-sensitive information, in particular on current news events. In a previous step they also try to automatically assess the newsworthiness of a discussion topic. This is to differenciate between the chatter and news related messages circulating on twitter in unsorted way. Both newsworthiness and credibility play an equally important role in this paper.

% =========================
% =========================
% =========================


\section{Twitter Case Study}
* from P1: Predicting information credibility in time-sensitive social media

* earthquake that hit Chile on February 2010

* Event characterization

* Twitter reaction: Information propagation behavior \& False rumor propagation


% =========================
% =========================
% =========================

\section{Proposed Method: Ranking of \\Tweets}
This section describes the method of ranking of tweets introduced in P2. 
Their \textit{learning to Rank} method is a data-driven approach to assign a score to tweet. 
For training the ranking model the RankSVM\cite{ranksvm} alogrithm is used.

Figure \ref{fig:p2overview} shows the the paradigm of the method and the separation of the trainign and test corpus.

\begin{figure}[h]
\centering
\epsfig{file=img/p2_overview.png, width=0.45\textwidth}
\caption{General Paradigm of Learning for Tweets Ranking}
\label{fig:p2overview}
\end{figure}

The learning to rank system used three types of featrues: Content relevance features, Twitter specific features and Account authority features

\subsubsection*{Content relevance features}
The content relevance features refer to those features which describe the content relevance between queries and tweets.
P1 uses three different types of content relavance:

\begin{itemize}
\item \b{Okapi BM25}\cite{okapi} score measures the relavance between query and tweet. It considers the length of a tweet, the average length of tweets and the Inverse Document Frequency.
\item \b{Similarity of contents} how many tweets of the query are similar in content with the current tweet. To accomplish that the cosine similarity between every pair of tweets is computed.
\item \b{Length} is simply measured by the number of words that a tweet contains.
\end{itemize}


\subsubsection*{Twitter Specific Features}
As the proposed method is tested on twitter it also relys on some of the Tweet-specific features. 

\begin{itemize}
\item \b{URL and URL Count} Tweets can contains one ore more URLs in the composed message. The feature URL hereby is a binary feature indicating with 1 if there is a URL present, whereas the URL Count extimates how often a particular URL is found in the tweet corpus.
\item \b{Retweet Count} Twitter users can forward a tweet to his or her followers with or without modification on the forwarded tweets. In this feature Retweet count is defined as the number of times a tweet is retweeted.
\item \b{Hash Tag Score} Hashtags are a type of label or metadata tag microblogging services, especially on twitter, which makes it easier for users to find messages with a specific theme or content. For this feature hashtags are collected from all tweets and sorted in descending order. The feature score correlates with the frequency of hashtags in the tweet with the most frequent hashtags.
\item \b{Reply} This is a binary feature. It is 1 when the tweet is a reply and 0 otherwise.
\item \b{Words out of vocabulary} This feature is used to roughly approximate the language quality of tweets. The vocabulary is defined by the words of a dictionary with 0.5 million entries.

\end{itemize}

\subsubsection*{Account Authority Features}
There are three important relations between users in Twitter: follow, retweet, and mention. 
In general user are seen as more authoritative if they have more followers, been mentioned in more tweets, listed in more lists and retweeted by more important users.

The main assumption here is that tweets are more likly to be informative if posted or retweeted by authoritative user.

To compute the authority of the user, P2 uses four different scores:
\begin{itemize}
\item \b{Follower score} number of followers a user has.
\item \b{Mention score} number of times a user is referred to in tweets.
\item \b{List score} number of lists a user appears in
\item \b{Popularity score} computed by PageRank algorithm (Page et al., 1999)\cite{pagerank} based on retweet relations.
\end{itemize}

\subsection*{Evaluation}
\subsubsection*{Dataset}
To test the proposed method the authors of P2 analyzed hot searches on CrowsEye within a week and collected the messages of 20 of the most frequent searched query terms. The terms included 5 persons 5 persons, 5 locations, 5 products and 5 movie names. 
In total there were 159,298 tweets captured in the time between Mach 25, 2010 and April 2, 2010.

For the generation of a test dataset 500 tweets of each query was labeled by a human editor. The evaluators could apply four judgments grades on the tweets depending on their relevance to the search query. 
The distribution of the grades were as follows: Excellent 20.9 \% ; Good 10.9\%; Fair 16.9\%; Bad 51.3\%.


\subsubsection*{Results}
For comparision the authors of P2 used the Normalized Discount Cumulative Gain (NDCG) metric, which is common to measure the quality of rankings.

The ranking model, RankSVM, was tested on different types of feature subsets. The main types were 
The main types of feature subsets proposed by the the authors are chronological order, account authority, and content relevance. Figure \ref{fig:ranking} shows the result of these rankings individually and also the combined or full set of features which is denoted as RankSVM\_Full.

\begin{figure}[h]
\centering
\epsfig{file=img/p2_results.png, width=0.45\textwidth}
\caption{Performance of Four Ranking Methods}
\label{fig:ranking}
\end{figure}

It is clearly visible that content relevance is the worst type, which should not come as a surprise as it is essential a re-ranking of the results of the Twitter search. It is also shown that account authority is useful for ranking tweet relevance, as it outperforms ranking through chonological order and compete with the model trained with all the features.

In the next part of the evaluation the authors of P2 tried to find out which feature as well as which combination of features showed the best results.
To complete this evaluation a greedy feature selection method was used to find the best feature cojunction whichs outperforms RankSVM\_full. The best feature conjunction was found to be composed of the features URL, Sum\_mention, First\_List, Length, and Important\_follower and denoted as RankSVM\_Best. It outperforms RankSVM\_full by about 15,3 \% at NDCG@10.

In the final evaluation it was tested which of the six features in particular had the most significant effect on RankSVM\_Best. Here the authors of P2 evaluate the importance of each feature by the decrement of performance when removing the feature measured from RankSVM\_Best. 
Figure \ref{fig:p2_resultFeature} shows that only the feature URL has a significant effect.

\begin{figure}[h]
\centering
\epsfig{file=img/p2_results_features.png, width=0.45\textwidth}
\caption{Importance of Each Feature}
\label{fig:p2_resultFeature}
\end{figure}


% =========================
% =========================
% =========================

\section{Proposed Method: Prediction\\ Model for Tweets}
P1 proposes a method to automatically distinguish between different typs of messages in time-sensitive social media like on Twitter.
The paper follows a supervised learning approach for the task of automatic classification of credible news events. A first classifier decides if an information cascade corresponds to a newsworthy event. Then a second classifier decides if this cascade can be considered credible or not.

% In Section 4 we present the process of creating a labeled data set of newsworthy events for credibility assessment. This procedure is performed using crowdsourcing tools

\subsection{Retrieving and Labeling of Data}
It should be noted that the work of P1 is based on previous works and is an information cascade, which is composed of all of the messages which usually accompany newsworthy events. This means that only aggregated values of a topic are computed and compared, not on user or message level.

For the collection of data on Twitter they used Twitter Monitor\cite{twittermonitor}, which detects sharp increases (bursts) in the frequency of sets of keywords found in messages. It provides a keyword-based query to filter messages to generate a subset of messages who caused the event detector to trigger.
During their 2 month of monitoring twitter, they collected, after some rough filtering, more than 2.500 topics with ofter 1.873.000 messages in total.

\subsubsection*{Newsworthiness}
The first step was to  separate newsworthy topics amoungst all the collected data. The task hereby was to distinguish between conversation or chat messages which have little importance outside a reduced circle of friends and newsworthy topics with the potential to be of interest to a borad set of people.

For the labeling process the authors of P1 used human editors and explored the crowdsourcing tool Mechanical Turk for the characterization of topics.
The human editors were asked if most of the messages were spreading news about a specific event (labeld as class NEWS) or mostly comments or conversation (labeld as class CHAT). To  reduce the effect of click spammers in the evaluation system the evaluators were asked to provide a sshort summary sentence of the topic.

In this process P1 selected randomly 383 topics with 221.279 in total. From this 383 topics 34.9 percent were labeld as CHAT, 29.5 percent as NEWS and 35.6 percent remained as UNSURE.


\subsubsection*{Credibility}
After the classification according to newsworthiness the topics in the NEWS class, aka the newsworthy topics, should be rated in a credibility aspect.
This second round of manual label takes the output of the previous manual classification as well as data from the automatic classifier build using the manually labeled data as input. With the use of the automatic classifier P1 was able to expand the amount of newsworithy topics to 747 cases.

The topics consisting of many tweets were again presented to a human evaluator for judgement. Again they were asked via an Mechanical Turk interface to rate the topics according to for topics: "almost certainly true", "likely to be false", "almost certainly false" or "uncertain". To reduce the effect of spammers they were again asked to provide a short justification sentence. 

The result of the credibility classes were as follows:
\begin{itemize}
\item 306 cases were "almost certainly true", 41 percent
\item 237 cases were "likely to be false", 31.8 percent
\item only 65 cases were "almost certainly false", 8.6 percent
\item 139 cases were "uncertain", 18.6 percent
\end{itemize}

\subsection{Prediction Model for Tweets}
The main hypothesis of P1 is that the level of newsworthiness and credibility can be estimated automatically. To accomplish that task they build to different classifier. In the following two sections it is described which factors are useful to establish  information credibility and which features are most useful for automatic estimation of newsworthiness and credibility.

%We believe that there are several factors that can be observed in the social media platform itself that are useful to establish information credibility. These factors include:
%* 1. reactions and emotion, opions, negative/positive sentiments 
%* 2. certainty, question info or not?
%* 3. sources cited, URLs, popular domain
%* 4. propagation characteristics, which user, how many follower etc.

\subsubsection*{Automatic discovery of newsworthy topics}
To train the supervised classifiers to determine if a set of tweets describes a newsworthy event or not the labeled data of the supervised training phase is uesed. As a reminder the labels consider the three classes NEWS, CHAT, UNSURE. Preliminary evaluations showed a negative effect of the UNSURE label, therefore it was decided to remove instances with the UNSURE label completly form the training data set.

\paragraph{Learning scheme}
P1 tested different machine learning schemes to run on the data, they included Naive Bayes, Bayes Net, Logistic Regression, and Random Forest. The results of the comparison showed that Bayes Net was the best learning scheme in this scenario, although Random Forest performed almost equally well.

\paragraph{Feature subset}
After the testing of the different learning schemes P1 studied how different features  contribute in the prediction of newsworthy topics.
The following subsets were investigated:

\begin{itemize}
\item \b{Text-only subset} considers all the features of the message text. This includes for example the average length of the tweets,
sentiment-based features, features related to URLs or twitter-specific features like hashtags or user mentions. In total 20 different features are contained in this subset.
\item \b{User subset} considers all of the features which represent the social network of users. This includes metrics like number of friends or number of followers. This subset contains the seven features.
\item \b{Topic subset}  considers all of the topic features which include the fraction of tweets that contain one of these four elements (at topic level): most frequent URL, most frequent hashtag, most frequent user mention, and most frequent author
\item \b{Propagation subset} considers the propagation-based features plus the fraction of re-tweets and the total number of tweets. This subset contains the seven different features.
\end{itemize}

\begin{figure}[h]
\centering
\epsfig{file=img/p1_table_feature_subset.png, width=0.45\textwidth}
\caption{Results for different feature subsets}
\end{figure}

\paragraph{Feature selection}
With the manually labeled test data P1 were able to compare the effects of different feature subsets on the test data. A small part of the results of this comparision is shown in Figure \ref{fig:boxNews}.

\begin{figure}[h]
\centering
\epsfig{file=img/p1_box_news.png, width=0.45\textwidth}
\caption{Boxplots depiction the distribution of features best separateing newsworthy and chat topics}
\label{fig:boxNews}
\end{figure}

The analysis shown in Figure \ref{fig:boxNews} shows eight different features and their effect on newsworthiness. It is shown that authors with a description in their profile are more likly to propagate chat. 
Topics which have longer tweets tend to be more related to news topics. Newsworthy topics also share more URLs which belong to the top-100 most popular domains but contains less frown emoticons than chat related topics, which might be quite intuitive.
Also, news topics tend to have less hashtags than chat, and contain more URLs in general.


\subsubsection*{Credibility prediction}
The task of the second classifier is to assign a credibility level (or score) to a topic deemed newsworthy by the previous classifier.

\paragraph{Data selection}
The manual labeling process introduced 4 types of classes concerning credibility of topics:  "almost certainly true"," likely to be false",  "almost certainly false" and "uncertain". To train the classifier this problem was reduced to a binary classification were topics with the almost certainly true label are assigned to a new class CREDIBLE and all others combined in the class NON-CREDIBLE.
In total there were 152 topic instances of class CREDIBLE and 136 in the class NON-CREDIBLE which correspond to a class balance of 47.2/52.8 percent. The topics consists of  165,312 tweets in total. 


\paragraph{Learning scheme}
Again there was a need for a lerning scheme for the training of the classifier. Simular to the previous classifier different learning schemes were tested to check the performance on the credibility assesment. The results of the best schemes, Random forest, Logistic and Meta leraning are pictured in Figure \ref{fig:schemeCredibility}.

\begin{figure}[h]
\centering
\epsfig{file=img/p1_scheme_credibility.png, width=0.45\textwidth}
\caption{Results of different lerning algorithms with respect credibility assessment}
\label{fig:schemeCredibility}
\end{figure}

As the results shows the predictability of the problem seems to be very difficult. Especially the misclassification of not-credible topics as credible is significant and the moderate kappa-statistic values indicate that improvements over a random predictor are limited.

\paragraph{Feature selection}
Analogous to the detection of newsworthiness the credibility classifier was tested on different features and feature subsets. Some of these features are shown with their corresponding boxplots in Figure \ref{fig:boxCredibility}:

\begin{itemize}
\item AVG\_STAT\_CNT, the average number of tweets posted by authors of the tweets in the topic in the past
\item FR\_SENT\_POS, the fraction of tweets having a positive sentiment
\item FR\_SENT\_NEG, the fraction of tweets having a negative sentiment
\item FR\_EMOT\_SMILE, the fraction of tweets containing a “smiling” emoticons
\item FR\_PRON\_FIRST, the fraction of tweets containing a first-person pronoun
\item FR\_PRON\_THIRD, the fraction of tweets containing a third-person pronoun
\item FR\_TWEETS\_URL, the fraction of tweets containing a URL
\item FR\_POP\_DOM\_TOP\_10000, the fraction of URLs pointing to a domain among the top 10,000 most visited ones
\item FR\_TWEETS\_QUEST\_EXCL\_MARK, the fraction of tweets containing a question or an exclamation mark
\end{itemize}


\begin{figure}[h]
\centering
\epsfig{file=img/p1_box_credibility.png, width=0.45\textwidth}
\caption{Boxplots dipicting distribution of different features, A represents credible, B represents non-credible topics}
\label{fig:boxCredibility}
\end{figure}

The analysis of the feature selection showed that tweets with more URLs and in particular URLs which are included on the top-10.000 most visited domains belongs to credible topics. 
Regarding polarity, non-credible tweets tend to concentrate more positive polarity scores, as opposite to credible tweets, which tend to express negative feelings.
In contrast to credible topics, authors of tweets classified as non-credible concentrate question and exclamation marks, frequently use first and third-person pronouns and include much more emoticons.


% =========================
% =========================
% =========================

\section{Comparision of the papers}


% =========================
% =========================
% =========================

\section{Conclusion}



\bibliographystyle{abbrv}
\bibliography{bibliography} 

\balancecolumns

\end{document}
