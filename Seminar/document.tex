\documentclass{proseminar}

\let\i\undefined
\let\c\undefined
\let\b\undefined
\newcommand{\i}[1]{\emph{#1}}
\newcommand{\c}[1]{\texttt{#1}}
\newcommand{\b}[1]{\textbf{#1}}
\begin{document}

\conferenceinfo{Albert-Ludwigs Universit\"at Freiburg\\Technische Fakult\"at, Institut f\"ur Informatik\\Lehrstuhl f\"ur Datenbanken \& Informationssysteme}{}

\title{Trust and Privacy in Social Media \\
\huge Content oriented trust}

\numberofauthors{1} 
\author{
Tim Schmiedl\\
\email{tim.schmiedl@neptun.uni-freiburg.de}
}

\maketitle

\section{Introduction}
Over the last couple of years microblogging became a well-established paradigm for interaction in online social networks.
Especially the success of Twitter let to an increase of huge amount of real-time informations on the internet. It lets user post short messages which are shown immeadietly for their followers. This concept was of course quickly adapted by news agencies, product pages and many other. So nowadays Twitter displays a wide variety of content. Most tweets correspond to the following categories; conversation items, wich are just valuable to user and its immediate circle of friends, or infomation or news items, which could be interesting for a broader community.

%* mobile internet devices 
%* propagating news and information about developing events 

% =========================

\subsection*{Current Situation}
Twitter was well known to display tweets in reverse cronical order displaying what is happening right now on the very top of the news feed. 
The microblogging company itself tried to experiement in some ways to show the information in different ways, one is to show popular tweets on top regardless of the posting time.
This decision was often critized as it often shows for the user unrelevant information or advertisment. This position of popular tweet was also often the target of internet trolls or spammers trying polute the news feed of many users.

As microblogging has become more and more popular it was shown that beside chatting about events on a day-to-day basis it can be in particularly helpful during emergency and/or crisis situations.
Twitter provide real-time information from the actual location where the crisis is unfolding. This information often spreads faster and to a wider audience than what traditional news media sources can achieve.


% =========================

\subsection*{Problems}
Twitters ranking method of displaying popular tweets as not taken into account content relevance or the twitter account.
This lets a large amount of pointless tweets may flood the relevant tweets of the users news feed. It also allows for more potential for spam to spead, as it is not filtered in this ranking method.

As a result of the big variety of content and different kinds of users it is very difficult to differenciate between news related topics and chat between user. Finding tweets which have valuable information outside its immediate circle of friends or follower is often not easy as there is no distinction between different kind of messages. 


% =========================

\subsection*{Goals of the Papers}
The primary element of this report are the two papers...

In P2 they propose a method to rank tweets based on their content relevance to a given query. They use different set of features to find out weather a tweet is interesting for a user. Their \i{learning to rank} algorithm considers content relevance as well as account authority and twitter-specific features to rank tweets. In their evaluation they find out that especially if a URL is present in the tweet improves the relevance as well as that just the number of followers to an account doesn't directly correlates to bigger relevances of tweets from that author.

In P1 the authors had a the main focus on the credibility of time-sensitive information, in particular on current news events. In a previous step they also try to automatically assess the newsworthiness of a discussion topic. This is to differenciate between the chatter and news related messages circulating on twitter in unsorted way. Both newsworthiness and credibility play an equally important role in this paper.

% =========================
% =========================
% =========================


\section{Twitter Case Study}
* from P1: Predicting information credibility in time-sensitive social media

* earthquake that hit Chile on February 2010

* Event characterization

* Twitter reaction: Information propagation behavior \& False rumor propagation


% =========================
% =========================
% =========================

\section{Proposed Method: Ranking of \\Tweets}
This section describes the method of ranking of tweets introduced in P2. 
Their \textit{learning to Rank} method is a data-driven approach to assign a score to tweet. 
For training the ranking model the RankSVM alogrith is used.

Figure \ref{fig:p2overview} shows the the paradigm of the method and the separation of the trainign and test corpus.

\begin{figure}[h]
\centering
\epsfig{file=img/p2_overview.png, width=0.45\textwidth}
\caption{General Paradigm of Learning for Tweets Ranking}
\label{fig:p2overview}
\end{figure}

The learning to rank system used three types of featrues: Content relevance features, Twitter specific features and Account authority features

\subsubsection*{Content relevance features}
The content relevance features refer to those features which describe the content relevance between queries and tweets.
P1 uses three different types of content relavance:

\begin{itemize}
\item \b{Okapi BM25} score measures the relavance between query and tweet. It considers the length of a tweet, the average length of tweets and the Inverse Document Frequency.
\item \b{Similarity of contents} how many tweets of the query are similar in content with the current tweet. To accomplish that the cosine similarity between every pair of tweets is computed.
\item \b{Length} is simply measured by the number of words that a tweet contains.
\end{itemize}


\subsubsection*{Twitter Specific Features}
As the proposed method is tested on twitter it also relys on some of the Tweet-specific features. 

\begin{itemize}
\item \b{URL and URL Count} Tweets can contains one ore more URLs in the composed message. The feature URL hereby is a binary feature indicating with 1 if there is a URL present, whereas the URL Count extimates how often a particular URL is found in the tweet corpus.
\item \b{Retweet Count} Twitter users can forward a tweet to his or her followers with or without modification on the forwarded tweets. In this feature Retweet count is defined as the number of times a tweet is retweeted.
\item \b{Hash Tag Score} Hashtags are a type of label or metadata tag microblogging services, especially on twitter, which makes it easier for users to find messages with a specific theme or content. For this feature hashtags are collected from all tweets and sorted in descending order. The feature score correlates with the frequency of hashtags in the tweet with the most frequent hashtags.
\item \b{Reply} This is a binary feature. It is 1 when the tweet is a reply and 0 otherwise.
\item \b{Words out of vocabulary} This feature is used to roughly approximate the language quality of tweets. The vocabulary is defined by the words of a dictionary with 0.5 million entries.

\end{itemize}

\subsubsection*{Account Authority Features}
+ There are three important relations between users in Twitter: follow, retweet, and mention. 

+ user are more authoritative: more followers, been mentioned in more tweets, listed in more lists and retweeted by more important users

+ tweet is more likly to be informative if posted or retweeted by authoritative user

+ Scores: Follower, Mention, List, Popularity (computed by PageRank)

% =========================
% =========================
% =========================

\section{Proposed Method: Prediction\\ Model for Tweets}
P1 proposes a method to automatically distinguish between different typs of messages in time-sensitive social media like on Twitter.
The paper follows a supervised learning approach for the task of automatic classification of credible news events. A first classifier decides if an information cascade corresponds to a newsworthy event. Then a second classifier decides if this cascade can be considered credible or not.

% In Section 4 we present the process of creating a labeled data set of newsworthy events for credibility assessment. This procedure is performed using crowdsourcing tools

\subsection{Retrieving and Labeling of Data}
It should be noted that the work of P1 is based on previous works and is an information cascade, which is composed of all of the messages which usually accompany newsworthy events. This means that only aggregated values of a topic are computed and compared, not on user or message level.

For the collection of data on Twitter they used Twitter Monitor, which detects sharp increases (bursts) in the frequency of sets of keywords found in messages. It provides a keyword-based query to filter messages to generate a subset of messages who caused the event detector to trigger.
During their 2 month of monitoring twitter, they collected, after some rough filtering, more than 2.500 topics with ofter 1.873.000 messages in total.

\subsubsection*{Newsworthiness}
The first step was to  separate newsworthy topics amoungst all the collected data. The task hereby was to distinguish between conversation or chat messages which have little importance outside a reduced circle of friends and newsworthy topics with the potential to be of interest to a borad set of people.

For the labeling process the authors of P1 used human editors and explored the crowdsourcing tool Mechanical Turk for the characterization of topics.
The human editors were asked if most of the messages were spreading news about a specific event (labeld as class NEWS) or mostly comments or conversation (labeld as class CHAT). To  reduce the effect of click spammers in the evaluation system the evaluators were asked to provide a sshort summary sentence of the topic.

In this process P1 selected randomly 383 topics with 221.279 in total. From this 383 topics 34.9 percent were labeld as CHAT, 29.5 percent as NEWS and 35.6 percent remained as UNSURE.


\subsubsection*{Credibility}
After the classification according to newsworthiness the topics in the NEWS class, aka the newsworthy topics, should be rated in a credibility aspect.
This second round of manual label takes the output of the previous manual classification as well as data from the automatic classifier build using the manually labeled data as input. With the use of the automatic classifier P1 was able to expand the amount of newsworithy topics to 747 cases.

The topics consisting of many tweets were again presented to a human evaluator for judgement. Again they were asked via an Mechanical Turk interface to rate the topics according to for topics: "almost certainly true", "likely to be false", "almost certainly false" or "uncertain". To reduce the effect of spammers they were again asked to provide a short justification sentence. 

The result of the credibility classes were as follows:
\begin{itemize}
\item 306 cases were "almost certainly true", 41 percent
\item 237 cases were "likely to be false", 31.8 percent
\item only 65 cases were "almost certainly false", 8.6 percent
\item 139 cases were "uncertain", 18.6 percent
\end{itemize}

\subsection{Prediction Model for Tweets}
The main hypothesis of P1 is that the level of newsworthiness and credibility can be estimated automatically. To accomplish that task they build to different classifier. In the following two sections it is described which factors are useful to establish  information credibility and which features are most useful for automatic estimation of newsworthiness and credibility.

%We believe that there are several factors that can be observed in the social media platform itself that are useful to establish information credibility. These factors include:
%* 1. reactions and emotion, opions, negative/positive sentiments 
%* 2. certainty, question info or not?
%* 3. sources cited, URLs, popular domain
%* 4. propagation characteristics, which user, how many follower etc.

\subsubsection*{Automatic discovery of newsworthy topics}
To train the supervised classifiers to determine if a set of tweets describes a newsworthy event or not the labeled data of the supervised training phase is uesed. As a reminder the labels consider the three classes NEWS, CHAT, UNSURE. Preliminary evaluations showed a negative effect of the UNSURE label, therefore it was decided to remove instances with the UNSURE label completly form the training data set.

\paragraph{Learning scheme}
P1 tested different machine learning schemes to run on the data, they included Naive Bayes, Bayes Net, Logistic Regression, and Random Forest. The results of the comparison showed that Bayes Net was the best learning scheme in this scenario, although Random Forest performed almost equally well.

\paragraph{Feature subset}
After the testing of the different learning schemes P1 studied how different features  contribute in the prediction of newsworthy topics.
The following subsets were investigated:

\begin{itemize}
\item \b{Text-only subset} considers all the features of the message text. This includes for example the average length of the tweets,
sentiment-based features, features related to URLs or twitter-specific features like hashtags or user mentions. In total 20 different features are contained in this subset.
\item \b{User subset} considers all of the features which represent the social network of users. This includes metrics like number of friends or number of followers. This subset contains the seven features.
\item \b{Topic subset}  considers all of the topic features which include the fraction of tweets that contain one of these four elements (at topic level): most frequent URL, most frequent hashtag, most frequent user mention, and most frequent author
\item \b{Propagation subset} considers the propagation-based features plus the fraction of re-tweets and the total number of tweets. This subset contains the seven different features.
\end{itemize}

\begin{figure}[h]
\centering
\epsfig{file=img/p1_table_feature_subset.png, width=0.45\textwidth}
\caption{Results for different feature subsets}
\end{figure}

\paragraph{Feature selection}
With the manually labeled test data P1 were able to compare the effects of different feature subsets on the test data. A small part of the results of this comparision is shown in Figure \ref{fig:boxNews}.

\begin{figure}[h]
\centering
\epsfig{file=img/p1_box_news.png, width=0.45\textwidth}
\caption{Boxplots depiction the distribution of features best separateing newsworthy and chat topics}
\label{fig:boxNews}
\end{figure}

The analysis shown in Figure \ref{fig:boxNews} shows eight different features and their effect on newsworthiness. It is shown that authors with a description in their profile are more likly to propagate chat. 
Topics which have longer tweets tend to be more related to news topics. Newsworthy topics also share more URLs which belong to the top-100 most popular domains but contains less frown emoticons than chat related topics, which might be quite intuitive.
Also, news topics tend to have less hashtags than chat, and contain more URLs in general.


\subsubsection*{Credibility prediction}
The task of the second classifier is to assign a credibility level (or score) to a topic deemed newsworthy by the previous classifier.

\paragraph{Data selection}
The manual labeling process introduced 4 types of classes concerning credibility of topics:  "almost certainly true"," likely to be false",  "almost certainly false" and "uncertain". To train the classifier this problem was reduced to a binary classification were topics with the almost certainly true label are assigned to a new class CREDIBLE and all others combined in the class NON-CREDIBLE.
In total there were 152 topic instances of class CREDIBLE and 136 in the class NON-CREDIBLE which correspond to a class balance of 47.2/52.8 percent. The topics consists of  165,312 tweets in total. 


\paragraph{Learning scheme}
Again there was a need for a lerning scheme for the training of the classifier. Simular to the previous classifier different learning schemes were tested to check the performance on the credibility assesment. The results of the best schemes, Random forest, Logistic and Meta leraning are pictured in Figure \ref{fig:schemeCredibility}.

\begin{figure}[h]
\centering
\epsfig{file=img/p1_scheme_credibility.png, width=0.45\textwidth}
\caption{Results of different lerning algorithms with respect credibility assessment}
\label{fig:schemeCredibility}
\end{figure}

As the results shows the predictability of the problem seems to be very difficult. Especially the misclassification of not-credible topics as credible is significant and the moderate kappa-statistic values indicate that improvements over a random predictor are limited.

\paragraph{Feature selection}
Analogous to the detection of newsworthiness the credibility classifier was tested on different features and feature subsets. Some of these features are shown with their corresponding boxplots in Figure \ref{fig:boxCredibility}:

\begin{itemize}
\item AVG\_STAT\_CNT, the average number of tweets posted by authors of the tweets in the topic in the past
\item FR\_SENT\_POS, the fraction of tweets having a positive sentiment
\item FR\_SENT\_NEG, the fraction of tweets having a negative sentiment
\item FR\_EMOT\_SMILE, the fraction of tweets containing a “smiling” emoticons
\item FR\_PRON\_FIRST, the fraction of tweets containing a first-person pronoun
\item FR\_PRON\_THIRD, the fraction of tweets containing a third-person pronoun
\item FR\_TWEETS\_URL, the fraction of tweets containing a URL
\item FR\_POP\_DOM\_TOP\_10000, the fraction of URLs pointing to a domain among the top 10,000 most visited ones
\item FR\_TWEETS\_QUEST\_EXCL\_MARK, the fraction of tweets containing a question or an exclamation mark
\end{itemize}


\begin{figure}[h]
\centering
\epsfig{file=img/p1_box_credibility.png, width=0.45\textwidth}
\caption{Boxplots dipicting distribution of different features, A represents credible, B represents non-credible topics}
\label{fig:boxCredibility}
\end{figure}




* Feature selection: evaluate 684 subsets, best results with 16 features

* fraction of tweets having a positive/negative sentiment; 

* fraction of tweets with URL, URL top 10.000 urls

* tweets containing: question mark, exclamation mark, first- /second-person pronoun, emoticons





% =========================
% =========================
% =========================

\section{Evaluation}
\subsection{Predicting information credibility in time-sensitive social media}

\subsubsection*{Credibility}
* predictability of the problem is very difficult, with very moderated k-statistic values

* main conclusions are that misclassification of not-credible topics as credible is significant, but recall rates are quite acceptable.

* features positive for credibility: more friends, more URLs + urls in popular 10.000, longer in general, negative sentiments

* features negative for credibility: positive polarity, more question and exclamation marks, first and third-persion pronouns

% =========================

\subsection{An Empirical Study on Learning to Rank of Tweets}
\subsubsection*{Dataset}
+ 20 query terms on CrowdEye (5 persons, 5 locations, 5 products and 5 movie names)

+ 159,298 tweets

+ 500 tweets/topic evaluated by human editor: labeled with grade 4 judgements 

+ Excellent 20.9 \% ; Good 10.9\%; Fair 16.9\%; Bad 51.3\% 

\subsubsection*{Results}
+ different methods tested in Five-fold cross-validation of tweets in 16 queries: chronological order, account authority, and content relevance + combinded
\begin{figure}[h]
\centering
\epsfig{file=img/p2_results.png, width=0.45\textwidth}
\caption{Performance of Four Ranking Methods}
\end{figure}

+ evaluation of the Feature Selection: which are the best features?

+ Combined set of features with best results: URL, Sum\_mention, First\_List, Length, and Important\_follower

+ tested with leave out one, results in figure 3

+ URL is best feature, other features on its own are not significant

\begin{figure}[h]
\centering
\epsfig{file=img/p2_results_features.png, width=0.45\textwidth}
\caption{Importance of Each Feature}
\end{figure}

% =========================
% =========================
% =========================

%\section{Related Work}

% =========================
% =========================
% =========================

\section{Conclusion}



\bibliographystyle{abbrv}
\bibliography{bibliography} 

\balancecolumns

\end{document}
